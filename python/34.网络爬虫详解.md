---

## 网络爬虫详解

网络爬虫（Web crawler / spider）是自动从互联网上抓取网页信息的程序。Python因其简单且强大的网络编程库，成为爬虫开发的首选语言。

---

### 一、网络爬虫基础

#### 1. 工作流程

1. **发送HTTP请求**  
   向目标网站发送请求，获取网页内容。

2. **解析网页内容**  
   取出网页中的数据，常用HTML、JSON解析。

3. **数据存储**  
   将提取的数据存入数据库、文件等。

4. **循环抓取/翻页处理**  
   自动跟踪链接，实现广泛抓取。

#### 2. 需要注意的点

- 尊重网站的`robots.txt`规则。
- 控制抓取频率，避免给服务器造成压力（反爬机制）。
- 合法合规，避免侵犯版权或隐私。

---

### 二、Python常用爬虫库

#### 1. requests

- 功能强大的HTTP库，简洁易用，支持发送各种HTTP请求。
- 一般负责发送请求，获取网页源码。

```python
import requests

response = requests.get('https://www.example.com')
print(response.text)  # 网页源码
```

#### 2. BeautifulSoup (bs4)

- HTML/XML解析库，简单高效。
- 支持多种解析器（如lxml），方便提取网页标签和数据。

```python
from bs4 import BeautifulSoup

html_doc = '<html><head><title>Example</title></head><body><p>Paragraph</p></body></html>'
soup = BeautifulSoup(html_doc, 'html.parser')
print(soup.title.string)  # 输出：Example
```

#### 3. lxml

- 高性能XML和HTML解析库。
- 兼容XPath查询语法，定位元素更强大。

```python
from lxml import etree

html = '<html><body><div><a href="url">link</a></div></body></html>'
tree = etree.HTML(html)
print(tree.xpath('//a/@href'))  # 输出：['url']
```

#### 4. Scrapy

- 功能完善的全功能异步爬虫框架，支持分布式爬虫。
- 集成请求调度、页面解析、数据存储、自动去重。
- 适合复杂项目、海量数据采集。

项目结构清晰，有Spider、Item、Pipeline等组件。

```bash
pip install scrapy
```

- 简单Spider示例：

```python
import scrapy

class ExampleSpider(scrapy.Spider):
    name = 'example'
    start_urls = ['https://www.example.com']

    def parse(self, response):
        title = response.css('title::text').get()
        yield {'title': title}
```

运行：

```bash
scrapy crawl example
```

#### 5. Selenium

- 浏览器自动化工具，用于爬取动态加载内容（JavaScript渲染）。
- 能操作浏览器，支持点击、滚动、等待，适合SPA站点。

```python
from selenium import webdriver

driver = webdriver.Chrome()
driver.get('https://www.example.com')
print(driver.page_source)
driver.quit()
```

#### 6. aiohttp & asyncio

- 异步HTTP客户端，用于高并发爬取。
- 结合Python内置异步库`asyncio`，提升IO密集型爬虫效率。

```python
import aiohttp
import asyncio

async def fetch(session, url):
    async with session.get(url) as response:
        return await response.text()

async def main():
    async with aiohttp.ClientSession() as session:
        html = await fetch(session, 'https://www.example.com')
        print(html)

asyncio.run(main())
```

---

### 三、爬虫分类

#### 1. 静态爬虫

- 直接请求HTML页面并解析。
- 适合网页源代码静态展示的网站。

#### 2. 动态爬虫

- 针对JavaScript渲染的数据，需要浏览器模拟或API反向解析。
- 可使用Selenium，Pyppeteer，Playwright等。

#### 3. 增量爬虫

- 只抓取更新或新增内容，常配合数据库做数据比对。

#### 4. 分布式爬虫

- 多台机器协作分布抓取，解决单机瓶颈。
- Scrapy结合Scrapy-Redis支持。

---

### 四、爬虫进阶技巧

#### 1. 伪装 (User-Agent, Headers)

- 模拟浏览器请求，避免被反爬网站屏蔽。

```python
headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}
response = requests.get('https://example.com', headers=headers)
```

#### 2. 代理IP

- 通过代理服务器发起请求，避免IP封禁。

```python
proxies = {'http': 'http://10.10.1.10:3128', 'https': 'https://10.10.1.10:1080'}
requests.get('https://example.com', proxies=proxies)
```

#### 3. Cookies和Session管理

- 维护登录状态，处理需要登录才能访问的页面。

```python
session = requests.Session()
session.post('https://example.com/login', data=login_data)
response = session.get('https://example.com/protected')
```

#### 4. 异步爬取

- 使用`aiohttp`或者`Scrapy`的异步功能实现高并发抓取。

#### 5. 数据清洗与持久化

- 抓取数据后，需做清洗文本、格式转换等。
- 持久化保存到文件（JSON, CSV）、数据库（MySQL, MongoDB）或搜索引擎（Elasticsearch）。

---

### 五、简单示例 - 用requests+BeautifulSoup爬取新闻标题

```python
import requests
from bs4 import BeautifulSoup

url = 'https://news.ycombinator.com/'
headers = {'User-Agent': 'Mozilla/5.0'}

response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.text, 'html.parser')

titles = soup.select('.storylink')
for idx, title in enumerate(titles, 1):
    print(f"{idx}. {title.text}")
```

运行即可打印Hacker News首页文章标题。

---

### 六、总结

| 爬虫类型   | 适用情况         | 代表库/框架               | 优点                         | 缺点                   |
| ---------- | ---------------- | ------------------------- | ---------------------------- | ---------------------- |
| 静态爬虫   | 纯静态页面       | requests + BS4/lxml       | 简单易用，需求低             | 处理不了JS渲染         |
| 动态爬虫   | JS渲染页面       | Selenium, Playwright      | 模拟真实浏览器，抓取动态数据 | 开销大，速度慢         |
| 高并发爬虫 | 巨量数据抓取     | Scrapy + Twisted，aiohttp | 异步协程、分布式，效率高     | 框架复杂，需要学习成本 |
| 分布式爬虫 | 大规模分布式采集 | Scrapy+Redis等            | 任务调度灵活，海量数据处理   | 复杂度高，环境配置复杂 |