好的，这是一个非常实际且重要的问题。将协同过滤应用于二元标签（如点击/未点击、购买/未购买、喜欢/不喜欢）的场景非常普遍，例如在新闻推荐、电商推荐中。

其核心思想不变，但**问题定义、代价函数和评价指标**需要相应调整。我们依然以吴恩达的优化方法（矩阵分解）为基础来讲解。

---

### 核心变化：从“评分预测”到“偏好概率预测”

*   **评分预测**：预测用户会给物品打多少分（如1-5星），是一个**回归问题**。
*   **二元标签**：预测用户是否会与物品产生交互（如点击），是一个**二分类问题**。

因此，我们的目标不再是预测一个具体的分数，而是预测用户 `u` 对物品 `i` 产生**正反馈（如点击）的概率**。

---

### 1. 问题形式化

**符号定义**：
*   `r(i, j)`： 指示函数。如果用户 `j` 与物品 `i` 有交互（正样本），则为 `1`；如果为“未交互”（负样本），则为 `0`。
    *   **注意**：在实际数据中，“未交互”可能包含两种可能：1）用户真的不喜欢；2）用户根本没看到。我们通常将所有“未交互”的项视为负样本，但要知道这会引入一些噪声。
*   `ŷ⁽ⁱ, ʲ⁾`： 模型预测的用户 `j` 对物品 `i` 产生正反馈的**概率**。
*   `θ⁽ʲ⁾`： 用户 `j` 的隐因子向量。
*   `x⁽ⁱ⁾`： 物品 `i` 的隐因子向量。

---

### 2. 模型与激活函数

在评分预测中，我们使用内积： `(θ⁽ʲ⁾)ᵀ x⁽ⁱ⁾`。
在二元标签中，我们需要将这个内积得分映射到一个 `(0, 1)` 之间的概率值。我们使用 **Sigmoid 函数**来实现这一点。

**预测模型**：
`ŷ⁽ⁱ, ʲ⁾ = σ( (θ⁽ʲ⁾)ᵀ x⁽ⁱ⁾ ) = 1 / (1 + exp(-(θ⁽ʲ⁾)ᵀ x⁽ⁱ⁾ ))`

现在，`ŷ⁽ⁱ, ʲ⁾` 就是一个介于0和1之间的值，表示用户喜欢的概率。

---

### 3. 代价函数：从均方误差到对数损失

回归问题我们用均方误差，二分类问题我们自然要用**对数损失**。

**代价函数 `J`**：

`J(x⁽¹⁾, ..., x⁽ⁿₘ⁾, θ⁽¹⁾, ..., θ⁽ⁿᵘ⁾) = - [ Σ_{(i, j)： r(i, j)=1} log(ŷ⁽ⁱ, ʲ⁾) + Σ_{(i, j)： r(i, j)=0} log(1 - ŷ⁽ⁱ, ʲ⁾) ] + (λ/2) [ Σ_{j=1}^{n_u} Σ_{k=1}^{n} (θ_k⁽ʲ⁾)² + Σ_{i=1}^{n_m} Σ_{k=1}^{n} (x_k⁽ⁱ⁾)² ]`

让我们来分解这个函数：

1.  **损失项**：`- [ Σ log(ŷ) + Σ log(1 - ŷ) ]`
    *   对于**正样本** (`r(i,j)=1`)，我们希望预测概率 `ŷ` 越大越好， `-log(ŷ)` 就越小。
    *   对于**负样本** (`r(i,j)=0`)，我们希望预测概率 `ŷ` 越小越好， `-log(1 - ŷ)` 就越小。
    *   这个损失函数完美地契合了二分类任务的目标。

2.  **正则化项**：`(λ/2) [ Σ Σ (θ_k⁽ʲ⁾)² + Σ Σ (x_k⁽ⁱ⁾)² ]`
    *   与之前完全相同，用于防止过拟合。

---

### 4. 优化算法：梯度下降

优化目标依然是最小化 `J`。我们仍然使用梯度下降，但偏导数的计算因Sigmoid函数而有所不同。

令 `z⁽ⁱ, ʲ⁾ = (θ⁽ʲ⁾)ᵀ x⁽ⁱ⁾`，则 `ŷ⁽ⁱ, ʲ⁾ = σ(z⁽ⁱ, ʲ⁾)`，且 `∂ŷ / ∂z = ŷ(1 - ŷ)`。

**计算偏导数**：

*   **对电影特征 `x_k⁽ⁱ⁾` 求导**：
    `∂J / ∂x_k⁽ⁱ⁾ = Σ_{j} [ (ŷ⁽ⁱ, ʲ⁾ - r(i, j)) * θ_k⁽ʲ⁾ ] + λ * x_k⁽ⁱ⁾`
    *   **这个形式非常优雅！** 对于每个 `(i, j)` 对，梯度是 `(预测值 - 真实值)` 乘以 `θ_k⁽ʲ⁾`。注意，这里的求和是对于所有用户 `j`（或者在实际编程中，我们只对有交互数据的 `(i, j)` 对进行计算）。

*   **对用户参数 `θ_k⁽ʲ⁾` 求导**：
    `∂J / ∂θ_k⁽ʲ⁾ = Σ_{i} [ (ŷ⁽ⁱ, ʲ⁾ - r(i, j)) * x_k⁽ⁱ⁾ ] + λ * θ_k⁽ʲ⁾`
    *   同样，梯度是 `(预测值 - 真实值)` 乘以 `x_k⁽ⁱ⁾`。

**最终的梯度下降更新规则**与之前形式一致，但内涵已变为二分类：

对于每个电影 `i` 和其特征维度 `k`：
`x_k⁽ⁱ⁾ := x_k⁽ⁱ⁾ - α * [ Σ_{j} ( (ŷ⁽ⁱ, ʲ⁾ - r(i, j)) * θ_k⁽ʲ⁾ ) + λ * x_k⁽ⁱ⁾ ]`

对于每个用户 `j` 和其参数维度 `k`：
`θ_k⁽ʲ⁾ := θ_k⁽ʲ⁾ - α * [ Σ_{i} ( (ŷ⁽ⁱ, ʲ⁾ - r(i, j)) * x_k⁽ⁱ⁾ ) + λ * θ_k⁽ʲ⁾ ]`

---

### 5. 推荐生成

模型训练好后，我们如何做推荐？

1.  **计算概率**：对于目标用户 `j`，计算他对所有候选物品 `i` 的偏好概率 `ŷ⁽ⁱ, ʲ⁾`。
2.  **排序**：将所有物品按照 `ŷ⁽ⁱ, ʲ⁾` 从高到低排序。
3.  **Top-N 推荐**：取出排名最高的 N 个物品作为推荐结果。

---

### 实践中的重要技巧

1.  **负样本采样**：
    *   真实场景中，正样本（点击）极少，负样本（未点击）极多。使用全部负样本计算梯度会非常慢。
    *   **解决方案**：对负样本进行**采样**，例如，为每个正样本采样3-10个负样本，与正样本混合成一个训练批次。这能大大加快训练速度且不影响效果。

2.  **隐式反馈的权重**：
    *   一次购买行为比一次点击行为更能代表用户偏好。
    *   **解决方案**：可以为不同的交互行为赋予不同的置信度权重 `c_ij`。例如，购买的权重 > 收藏的权重 > 点击的权重。然后将损失函数中的每一项乘以对应的 `c_ij`。

3.  **评估指标**：
    *   不再使用均方根误差，而是使用更适合排序任务的指标：
        *   **精确率 / 召回率 / F1分数**
        *   **AUC**
        *   **Mean Average Precision @K**
        *   **Normalized Discounted Cumulative Gain @K**

### 总结

将协同过滤应用于二元标签，主要完成了以下转变：

| 方面         | 评分预测                   | 二元标签                  |
| :----------- | :------------------------- | :------------------------ |
| **问题类型** | 回归                       | 二分类                    |
| **模型输出** | 评分                       | 偏好概率                  |
| **激活函数** | 无（或恒等函数）           | **Sigmoid**               |
| **代价函数** | 均方误差                   | **对数损失**              |
| **梯度形式** | `(预测分 - 真实分) * 特征` | `(预测概率 - 0/1) * 特征` |
| **评估指标** | RMSE, MAE                  | Precision, Recall, AUC    |

这种基于矩阵分解和对数损失的协同过滤，是处理隐式反馈（如点击流数据）的强大基础模型，也是许多更复杂模型（如BPR， eALS）的基石。